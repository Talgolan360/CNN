# -*- coding: utf-8 -*-
"""CNN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iIEusidngneWfbsC8RhE9DVhVTegLE4G

** * CNN**
"""



"""<img src="https://github.com/FarzadNekouee/Keras-CIFAR10-CNN-Model/blob/master/image.png?raw=true" width="1800">

<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:125%; text-align:left">

<h2 align="left"><font color=#8502d1>CIFAR-10 Dataset</font></h2>

The __CIFAR-10__ dataset is a well-established benchmark in the field of machine learning, specifically designed for __image classification__. Comprising __60,000 color images__, each of __size 32x32 pixels__, the dataset is segmented into __10 distinct classes__, each representing a different object or creature. The classes encompass the following:

- Airplane
- Automobile
- Bird
- Cat
- Deer
- Dog
- Frog
- Horse
- Ship
- Truck

Each class contains an equal distribution, boasting 6,000 images. From the total image count, 50,000 are designated for training while the remaining 10,000 are set aside for testing.

<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:125%; text-align:left">

<h2 align="left"><font color=#8502d1>Objectives</font></h2>
    
* __Data Insights and Exploration__
   - Familiarize with the CIFAR-10 dataset.
   - Visually inspect sample images from various classes to understand data distribution.

    
* __Comprehensive Data Preprocessing__
   - Normalize pixel values of the images to enhance model training efficiency.
   - Convert image labels into a one-hot encoded format suitable for classification tasks.
   - Implement data augmentation techniques to increase the dataset's variability and improve model generalization.

    
* __Architectural Design using Keras__
   - Design a Convolutional Neural Network (CNN) tailored for the CIFAR-10 dataset using the __Keras__ framework.
   - Incorporate mechanisms such as dropouts and regularizations to counteract overfitting.

    
* __Model Training Process__
   - Train the CNN using the prepared dataset.
   - Utilize callbacks to adjust the learning rate dynamically and halt the training early if no improvements are detected, restoring the best model weights from the training.

    
* __Learning Analysis__
   - Visualize the model's learning curves, observing both training and validation performance metrics over epochs.

    
* __Model Evaluation__
   - Assess the trained model's accuracy and loss on the unseen test data to determine its robustness.

    
* __Real-world Generalization Check__
   - Evaluate the model's predictive capability using an image not part of the CIFAR-10 dataset to gauge its real-world applicability.

<a id="contents_tabel"></a>    
<div style="border-radius:10px; padding: 15px; background-color: #e2c9ff; font-size:125%; text-align:left">

<h2 align="left"><font color=#8502d1>Table of Contents</font></h2>
    
* [Step 1 | Import Necessary Libraries](#import)
* [Step 2 | Data Preparation and Exploration](#exploration)
* [Step 3 | Data Preprocessing](#preprocessing)
    - [Step 3.1 | Normalization of Image Data](#normalization)
    - [Step 3.2 | One-Hot Encoding of Labels](#onehot)
    - [Step 3.3 | Data Augmentation](#augmentation)
* [Step 4 | Define CNN Model Architecture](#cnn)
* [Step 5 | Training the CNN Model](#train)
* [Step 6 | Visualizing the Learning Curves](#curves)
* [Step 7 | Evaluating the Optimal Model on Test Data](#evaluation)
* [Step 8 | Performance on an Out-of-Dataset Image](#image)

<a id="import"></a>
# <p style="background-color:#8502d1; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;">Step 1 | Import Necessary Libraries</p>

⬆️ [Tabel of Contents](#contents_tabel)

<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">

First, we're importing all the necessary libraries to kick off our project. We'll be relying on __TensorFlow__ and __Keras__ to handle the image data, craft our model, and optimize it for best performance:
"""

import warnings
warnings.filterwarnings('ignore')

import cv2
import numpy as np
import urllib.request
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from keras.datasets import cifar10
from keras.preprocessing.image import ImageDataGenerator
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D
from keras.layers import Dropout, Flatten, BatchNormalization
from keras.regularizers import l2
from keras.optimizers import Adam
from keras.callbacks import ReduceLROnPlateau, EarlyStopping
from keras.models import load_model

"""<a id="exploration"></a>
# <p style="background-color:#8502d1; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;">Step 2 | Data Preparation and Exploration</p>

⬆️ [Tabel of Contents](#contents_tabel)

<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">

First, let's download the __CIFAR-10__ dataset from Keras library:
"""

(X_train, y_train), (X_test, y_test) = cifar10.load_data()

X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=0)

"""<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">

Then, let's split original training data to training and validation sets:
"""

# CIFAR-10 classes
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

selected_classes = ['automobile', 'dog', 'frog']
selected_classes_indices = [class_names.index(cls) for cls in selected_classes]

# Generate boolean index arrays for all sets
train_filtered_indices = np.isin(y_train.squeeze(), selected_classes_indices)
valid_filtered_indices = np.isin(y_valid.squeeze(), selected_classes_indices)
test_filtered_indices = np.isin(y_test.squeeze(), selected_classes_indices)

# Apply filtering to all sets
X_train = X_train[train_filtered_indices]
y_train = y_train[train_filtered_indices]

X_valid = X_valid[valid_filtered_indices]
y_valid = y_valid[valid_filtered_indices]

X_test = X_test[test_filtered_indices]
y_test = y_test[test_filtered_indices]

"""<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">
    
Now, we're printing out the dimensions of our training, validation, and test datasets:
"""

print('Train Images Shape:      ', X_train.shape)
print('Train Labels Shape:      ', y_train.shape)

print('\nValidation Images Shape: ', X_valid.shape)
print('Validation Labels Shape: ', y_valid.shape)

print('\nTest Images Shape:       ', X_test.shape)
print('Test Labels Shape:       ', y_test.shape)

"""<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">

Afterward, let's take an overview of the __CIFAR-10__ dataset:
"""

# Create a new figure
plt.figure(figsize=(15,15))

# Loop over the first 25 images
for i in range(64):
    # Create a subplot for each image
    plt.subplot(8, 8, i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)

    # Display the image
    plt.imshow(X_train[i])

    # Set the label as the title
    plt.title(class_names[y_train[i][0]], fontsize=12)

# Display the figure
plt.show()

"""<a id="preprocessing"></a>
# <p style="background-color:#8502d1; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;">Step 3 | Data Preprocessing</p>

⬆️ [Tabel of Contents](#contents_tabel)

<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">

In the **Data Preprocessing** phase, we undertake essential preparatory measures to ensure our dataset is aptly primed for the modeling process:

1. **Normalization of Image Data**

2. **One-Hot Encoding of Labels**

3. **Data Augmentation**

<a id="normalization"></a>

# <b><span style='color:darkorange'>Step 3.1 |</span><span style='color: #8502d1'> Normalization of Image Data</span></b>

<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">
    
First of all, I am going to convert the pixel values data type to __float32__ type, and then normalizes them by subtracting the mean and dividing by the standard deviation of the training set, enhancing the model's training efficiency and effectiveness:
"""

# Convert pixel values data type to float32
X_train = X_train.astype('float32')
X_test  = X_test.astype('float32')
X_valid = X_valid.astype('float32')

# Calculate the mean and standard deviation of the training images
mean = np.mean(X_train)
std  = np.std(X_train)

# Normalize the data
# The tiny value 1e-7 is added to prevent division by zero
X_train = (X_train-mean)/(std+1e-7)
X_test  = (X_test-mean) /(std+1e-7)
X_valid = (X_valid-mean)/(std+1e-7)

"""<a id="onehot"></a>

# <b><span style='color:darkorange'>Step 3.2 |</span><span style='color: #8502d1'> One-Hot Encoding of Labels</span></b>

<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">

Then, I am going to convert the class labels to one-hot vectors to transform the categorical labels into a format suitable for multi-class classification by neural networks:
"""

y_train = to_categorical(y_train, 10)
y_valid = to_categorical(y_valid, 10)
y_test  = to_categorical(y_test, 10)

"""<a id="augmentation"></a>

# <b><span style='color:darkorange'>Step 3.3 |</span><span style='color: #8502d1'> Data Augmentation</span></b>

<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">

Finally, I am going to implement data augmentation to artificially expand the size of the training set by creating modified versions of images in the dataset. This helps improve the model's ability to generalize, thereby reducing overfitting. Data augmentation techniques such as rotations, shifts, flips, shearing, and intensity changes introduce small variations to the existing images, creating a broader set of training samples to learn from.

The choice of data augmentation techniques often depends on the specific characteristics of the dataset and the problem at hand. The __CIFAR-10__ dataset comprises small color images of objects from 10 different classes. Given the nature of these images, some augmentation techniques are more applicable than others:

* __Rotation__: A small degree of rotation can help the model become invariant to the orientation of the object. The `rotation_range=15` means the image could be rotated randomly within -15 to 15 degrees. However, large rotations could be harmful since the CIFAR-10 images are relatively small and a big rotation might put the object outside of the image.

    
* __Width and Height shift__: Small shifts can help the model become invariant to the position of the object in the image. Here, `width_shift_range=0.12` and `height_shift_range=0.12` mean the image could be moved horizontally or vertically by up to 12% of its width or height respectively. Again, since the images are small, large shifts might put the object outside of the image.

    
* __Horizontal Flip__: A horizontal flip is a sensible choice for this dataset because for many images, the object of interest remains the same when flipped horizontally (for example, a flipped car is still a car).

    
* __Zoom__: Small zooming in by up to 10% (`zoom_range=0.1`) can also help the model generalize better. However, excessive zooming might lead to significant information loss.

    
* __Brightness Range__: Changing brightness can simulate various lighting conditions. With `brightness_range=[0.9,1.1]`, the brightness of the image is randomly changed to a value between 90% and 110% of the original brightness.

    
* __Shear Intensity__: With `shear_range=10`, a shear intensity within the range of -10 to +10 degrees is applied. This transformation slants the shape of the image, helping the model to recognize objects in different perspectives.

    
* __Channel Shift Intensity__: With `channel_shift_range=0.1`, the intensities of the RGB channels are randomly shifted by up to 10% of their full scale. This can help the model handle different lighting conditions and color variations.

While some augmentation techniques like vertical flips and color jittering may not be appropriate for all classes in the __CIFAR-10__ dataset, the chosen techniques are expected to help improve the robustness and generalization capability of the model.
"""

# Data augmentation
data_generator = ImageDataGenerator(
    # Rotate images randomly by up to 15 degrees
    rotation_range=15,

    # Shift images horizontally by up to 12% of their width
    width_shift_range=0.12,

    # Shift images vertically by up to 12% of their height
    height_shift_range=0.12,

    # Randomly flip images horizontally
    horizontal_flip=True,

    # Zoom images in by up to 10%
    zoom_range=0.1,

    # Change brightness by up to 10%
    brightness_range=[0.9,1.1],

    # Shear intensity (shear angle in counter-clockwise direction in degrees)
    shear_range=10,

    # Channel shift intensity
    channel_shift_range=0.1,
)

"""<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">
    
When running the above code, we are setting up a pipeline for preprocessing the images during model training. The generator applies a series of random transformations (specified by the parameters) to the images each time they are loaded into the model for training. Each time an epoch is run during model training, these random transformations will create different variations of the original images. These changes are made __on-the-fly__ and don't modify our original dataset.

<a id="cnn"></a>
# <p style="background-color:#8502d1; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;">Step 4 | Define CNN Model Architecture</p>

⬆️ [Tabel of Contents](#contents_tabel)

<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">

The model architecture I am going to define is inspired from the __VGG16__ network. It contains multiple convolutional layers followed by max-pooling and dropout layers, and finally a fully connected layer for classification. While not adopting advanced modules like residuals or inceptions, this design is simpler, ensuring fewer parameters and a more straightforward architecture, making it more computationally efficient.

Here is a brief explanation of the architecture:

- The network begins with __a pair of Conv2D layers__, each with __32 filters of size 3x3__. This is followed by a __Batch Normalization__ layer which accelerates training and provides some level of __regularization__, helping to prevent overfitting.

    
- The pairs of Conv2D layers are followed by a __MaxPooling2D layer__, which reduces the spatial dimensions (height and width), effectively providing a form of translation invariance and reducing computational complexity. This is followed by a __Dropout layer__ that randomly sets a fraction (0.2 for the first dropout layer) of the input units to 0 at each update during training, helping to prevent overfitting.

    
- This pattern of two Conv2D layers, followed by a Batch Normalization layer, a MaxPooling2D layer, and a Dropout layer, repeats three more times. The number of filters in the Conv2D layers doubles with each repetition, starting from 32 and going up to 64, 128, and then 256. This increasing pattern helps the network to learn more complex features at each level. The dropout rate also increases at each step, from 0.2 to 0.5.

    
- After the convolutional and pooling layers, a __Flatten layer__ is used to convert the 2D outputs of the preceding layer into a 1D vector.

    
- Finally, a __Dense (or fully connected) layer__ is used for classification. It has 10 units, each representing one of the 10 classes of the CIFAR-10 dataset, and a __softmax activation function__ is used to convert the outputs to probability scores for each class.

    
This architecture leverages the strengths of deep CNNs to learn hierarchical features from the CIFAR-10 images. Regularization techniques such as __L2 regularization__, __Dropout__, and __Batch Normalization__ are also used to combat overfitting. While being inspired by VGG16, the model remains simpler and does not incorporate the more advanced features of recent architectures, focusing instead on efficiency and simplicity.

#First model - including the seventh and eight convolutional layer, with data augmentation
"""

# Initialize a sequential model
model_1 = Sequential()

# Set the weight decay value for L2 regularization
weight_decay = 0.0001

# Add the first convolutional layer with 32 filters of size 3x3
model_1.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay),
                 input_shape=X_train.shape[1:]))
# Add batch normalization layer
model_1.add(BatchNormalization())

# Add the second convolutional layer similar to the first
model_1.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_1.add(BatchNormalization())

# Add the first max pooling layer with pool size of 2x2
model_1.add(MaxPooling2D(pool_size=(2, 2)))
# Add dropout layer with 0.2 dropout rate
model_1.add(Dropout(rate=0.2))

# Add the third and fourth convolutional layers with 64 filters
model_1.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_1.add(BatchNormalization())
model_1.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_1.add(BatchNormalization())

# Add the second max pooling layer and increase dropout rate to 0.3
model_1.add(MaxPooling2D(pool_size=(2, 2)))
model_1.add(Dropout(rate=0.3))

# Add the fifth and sixth convolutional layers with 128 filters
model_1.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_1.add(BatchNormalization())
model_1.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_1.add(BatchNormalization())

# Add the third max pooling layer and increase dropout rate to 0.4
model_1.add(MaxPooling2D(pool_size=(2, 2)))
model_1.add(Dropout(rate=0.4))

# Add the seventh and eighth convolutional layers with 256 filters
model_1.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_1.add(BatchNormalization())
model_1.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_1.add(BatchNormalization())

# Add the fourth max pooling layer and increase dropout rate to 0.5
model_1.add(MaxPooling2D(pool_size=(2, 2)))
model_1.add(Dropout(rate=0.5))

# Flatten the tensor output from the previous layer
model_1.add(Flatten())

# Add a fully connected layer with softmax activation function for outputting class probabilities
model_1.add(Dense(10, activation='softmax'))

"""#Second model - including the seventh and eight convolutional layer, without data augmentation"""

# Initialize a sequential model
model_2 = Sequential()

# Set the weight decay value for L2 regularization
weight_decay = 0.0001

# Add the first convolutional layer with 32 filters of size 3x3
model_2.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay),
                 input_shape=X_train.shape[1:]))
# Add batch normalization layer
model_2.add(BatchNormalization())

# Add the second convolutional layer similar to the first
model_2.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_2.add(BatchNormalization())

# Add the first max pooling layer with pool size of 2x2
model_2.add(MaxPooling2D(pool_size=(2, 2)))
# Add dropout layer with 0.2 dropout rate
model_2.add(Dropout(rate=0.2))

# Add the third and fourth convolutional layers with 64 filters
model_2.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_2.add(BatchNormalization())
model_2.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_2.add(BatchNormalization())

# Add the second max pooling layer and increase dropout rate to 0.3
model_2.add(MaxPooling2D(pool_size=(2, 2)))
model_2.add(Dropout(rate=0.3))

# Add the fifth and sixth convolutional layers with 128 filters
model_2.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_2.add(BatchNormalization())
model_2.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_2.add(BatchNormalization())

# Add the third max pooling layer and increase dropout rate to 0.4
model_2.add(MaxPooling2D(pool_size=(2, 2)))
model_2.add(Dropout(rate=0.4))

# Add the seventh and eighth convolutional layers with 256 filters
model_2.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_2.add(BatchNormalization())
model_2.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_2.add(BatchNormalization())

# Add the fourth max pooling layer and increase dropout rate to 0.5
model_2.add(MaxPooling2D(pool_size=(2, 2)))
model_2.add(Dropout(rate=0.5))

# Flatten the tensor output from the previous layer
model_2.add(Flatten())

# Add a fully connected layer with softmax activation function for outputting class probabilities
model_2.add(Dense(10, activation='softmax'))

"""#Third model - Without the seventh and eight convolutional layer, with data augmentation"""

# Initialize a sequential model
model_3 = Sequential()

# Set the weight decay value for L2 regularization
weight_decay = 0.0001

# Add the first convolutional layer with 32 filters of size 3x3
model_3.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay),
                 input_shape=X_train.shape[1:]))
# Add batch normalization layer
model_3.add(BatchNormalization())

# Add the second convolutional layer similar to the first
model_3.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_3.add(BatchNormalization())

# Add the first max pooling layer with pool size of 2x2
model_3.add(MaxPooling2D(pool_size=(2, 2)))
# Add dropout layer with 0.2 dropout rate
model_3.add(Dropout(rate=0.2))

# Add the third and fourth convolutional layers with 64 filters
model_3.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_3.add(BatchNormalization())
model_3.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_3.add(BatchNormalization())

# Add the second max pooling layer and increase dropout rate to 0.3
model_3.add(MaxPooling2D(pool_size=(2, 2)))
model_3.add(Dropout(rate=0.3))

# Add the fifth and sixth convolutional layers with 128 filters
model_3.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_3.add(BatchNormalization())
model_3.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu', kernel_regularizer=l2(weight_decay)))
model_3.add(BatchNormalization())

# Add the third max pooling layer and increase dropout rate to 0.4
model_3.add(MaxPooling2D(pool_size=(2, 2)))
model_3.add(Dropout(rate=0.4))

# Flatten the tensor output from the previous layer
model_3.add(Flatten())

# Add a fully connected layer with softmax activation function for outputting class probabilities
model_3.add(Dense(10, activation='softmax'))

"""# Summaries

<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">

Let's display the detailed architecture of the model:
"""

model_1.summary()

model_2.summary()

model_3.summary()

"""<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">

Our model consists of __1,186,346 parameters__, of which 1,184,426 are trainable. This is __a relatively compact model__, especially when compared to advanced architectures which often have tens or even hundreds of millions of parameters.

<a id="train"></a>
# <p style="background-color:#8502d1; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;">Step 5 | Training the CNN Model</p>

⬆️ [Tabel of Contents](#contents_tabel)

<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">

Now, I am going to train my neural network model. The training uses a batch size of 64 and will run for a maximum of 250 epochs or until the early stopping condition is met. During the training, the model's performance is evaluated on the validation data after each epoch. I've added a couple of callback functions to enhance the training process:

* The __ReduceLROnPlateau callback__ is used to reduce the learning rate by half (factor=0.5) whenever the validation loss does not improve for 10 consecutive epochs. This helps to adjust the learning rate dynamically, allowing the model to get closer to the global minimum of the loss function when progress has plateaued. This strategy can improve the convergence of the training process.
    

* The __EarlyStopping callback__ is employed to monitor the validation loss and halt the training process when there hasn't been any improvement for a certain number of epochs, ensuring that the model doesn't waste computational resources and time. Furthermore, this callback restores the best weights from the training process, ensuring we conclude with the optimal model configuration from the epochs.
"""

metrics_history = []

# Set the batch size for the training
batch_size = 64

# Set the maximum number of epochs for the training
epochs = 50

# Define the optimizer (Adam)
optimizer = Adam(learning_rate=0.0005)
#optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)


# Compile the model with the defined optimizer, loss function, and metrics
model_1.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
model_2.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
model_3.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

# Add ReduceLROnPlateau callback
# Here, the learning rate will be reduced by half (factor=0.5) if no improvement in validation loss is observed for 10 epochs
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=0.00001)

# Add EarlyStopping callback
# Here, training will be stopped if no improvement in validation loss is observed for 40 epochs.
# The `restore_best_weights` parameter ensures that the model weights are reset to the values from the epoch
# with the best value of the monitored quantity (in this case, 'val_loss').
early_stopping = EarlyStopping(monitor='val_loss', patience=40, restore_best_weights=True, verbose=1)

# Fit the model on the training data, using the defined batch size and number of epochs
# The validation data is used to evaluate the model's performance during training
# The callbacks implemented are learning rate reduction when a plateau is reached in validation loss and
# stopping training early if no improvement is observed
history1 = model_1.fit(data_generator.flow(X_train, y_train, batch_size=batch_size),
          epochs=epochs,
          validation_data=(X_valid, y_valid),
          callbacks=[reduce_lr, early_stopping],
          verbose=2)

metrics_history.append(history1)

history1 = model_2.fit(X_train, y_train,
                       batch_size=batch_size,
                       epochs=epochs,
                       validation_data=(X_valid, y_valid),
                       callbacks=[reduce_lr, early_stopping],
                       verbose=2)

metrics_history.append(history1)

history1 = model_3.fit(data_generator.flow(X_train, y_train, batch_size=batch_size),
          epochs=epochs,
          validation_data=(X_valid, y_valid),
          callbacks=[reduce_lr, early_stopping],
          verbose=2)

metrics_history.append(history1)

"""<a id="curves"></a>
# <p style="background-color:#8502d1; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;">Step 6 | Visualizing the Learning Curves</p>

⬆️ [Tabel of Contents](#contents_tabel)

<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">

Now I am going to generate plots for visualizing the training and validation loss, and accuracy evolution over epochs using model history:
"""

plt.figure(figsize=(15,6))

# Creates the accuracy graph for all 3 models
plt.subplot(1, 2, 1)
for i, history1 in enumerate(metrics_history):
    plt.plot(history1.history['val_accuracy'], label=f'Model {i+1}')
plt.title('Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Creates the loss graph for all 3 models
plt.subplot(1, 2, 2)
for i, history in enumerate(metrics_history):
    plt.plot(history.history['val_loss'], label=f'Model {i+1}')
plt.title('Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

"""<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">

Based on the visualizations above, it's evident that the model is performing well without signs of overfitting. This conclusion is supported by the close alignment of training and validation accuracy and loss values throughout the training process. The gap between training and validation accuracy remains minimal, indicating that the model generalizes well to unseen data. Similarly, the model's loss on validation data closely follows the training loss, reinforcing the assertion of good generalization. Therefore, the model appears to be well regularized and not overfitting to the training data.

<a id="evaluation"></a>
# <p style="background-color:#8502d1; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;">Step 7 | Evaluating the Optimal Model on Test Data</p>

⬆️ [Tabel of Contents](#contents_tabel)

<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">

As we have set `restore_best_weights=True` in EarlyStopping, after training, the model itself will have the best weights Following this, I will use this model to evaluate its performance on the test data, calculating the test accuracy and loss:

**We can see that model 2 is a bit higher in the accuracy results from model 1 and thats why this is our chosen model. The loss of Model 1 is lower than model 2 but we decided to go with the highest accuracy**
"""

# Use the model to make predictions, evaluate on test data
test_loss, test_acc = model_2.evaluate(X_test, y_test, verbose=1)

print('\nTest Accuracy:', test_acc)
print('Test Loss:    ', test_loss)

# Initialize lists to store indices of correct and incorrect predictions for each class
correct_indices = [[] for _ in range(10)]
incorrect_indices = [[] for _ in range(10)]

pred = model_2.predict(X_test)
predicted_labels = np.argmax(pred, axis=1)
true_labels = np.argmax(y_test, axis=1)

# Populate lists with indices
for i in range(len(true_labels)):
    if predicted_labels[i] == true_labels[i]:
        correct_indices[true_labels[i]].append(i)
    else:
        incorrect_indices[true_labels[i]].append(i)

plt.figure(figsize=(15, 15))
position = 1

for class_index in range(10):
    # Display correct prediction
    correct_index = correct_indices[class_index][0] if len(correct_indices[class_index]) > 0 else None
    if correct_index is not None:
        plt.subplot(5, 4, position)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(X_test[correct_index])
        plt.title(f'Predicted: {class_names[predicted_labels[correct_index]]}\nTrue: {class_names[true_labels[correct_index]]}')
        position += 1

    # Display incorrect prediction
    incorrect_index = incorrect_indices[class_index][0] if len(incorrect_indices[class_index]) > 0 else None
    if incorrect_index is not None:
        plt.subplot(5, 4, position)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(X_test[incorrect_index])
        plt.title(f'Predicted: {class_names[predicted_labels[incorrect_index]]}\nTrue: {class_names[true_labels[incorrect_index]]} (Incorrect)')
        position += 1

plt.show()

"""<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">

<h3 align="left"><font color=#8502d1>🎯 Inference:</font></h3>

With a test accuracy of __more than 90%__, our model demonstrates exceptional performance on unseen data. This high accuracy, achieved using __a relatively compact model of just about 1.2 million parameters__, is noteworthy. Many advanced architectures employ tens or even hundreds of millions of parameters to achieve similar or only slightly better results. The proximity of the test loss and accuracy to their respective training counterparts signifies that our model is not merely memorizing the training data but is genuinely understanding patterns and effectively generalizing from the training data to unseen data. Thus, it can be inferred that our model not only delivers reliable predictions but also strikes a balance between efficiency and performance.

<a id="image"></a>
# <p style="background-color:#8502d1; font-family:calibri; color:white; font-size:150%; text-align:center; border-radius:15px 50px;">Step 8 | Performance on an Out-of-Dataset Image</p>

⬆️ [Tabel of Contents](#contents_tabel)

<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">

To further explore the generalization capability of our trained CIFAR-10 classification model, I'll assess its performance using an external truck image. This image, which isn't part of the CIFAR-10 dataset, has been sourced from my [GitHub repository](https://github.com/FarzadNekouee/Keras-CIFAR10-CNN-Model/blob/master/truck_sample.png). It provides an opportunity to see how our model behaves with real-world, out-of-dataset samples:
"""

import requests
from PIL import Image
from io import BytesIO
import tensorflow as tf
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image

#Function for processing the images from the internet
def load_and_process_image(url):
    response = requests.get(url)
    img = Image.open(BytesIO(response.content))
    img = img.resize((32, 32)) # Resize to 32X32
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = preprocess_input(img_array)
    return img_array

# URLs for the photos we took from the internet
image_urls = [
    'https://news.automobile.tn/2024/02/le-kia-ev9-elu-meilleur-vehicule-electrique-de-lannee-aux-motorweek-drivers-choice-awards-2383_min.jpg',
    'https://www.hartz.com/wp-content/uploads/2022/04/small-dog-owners-1.jpg',
    'https://static.scientificamerican.com/sciam/cache/file/41DF7DA0-EE58-4259-AA815A390FB37C55_source.jpg'
]

for i, url in enumerate(image_urls):
    img_array = load_and_process_image(url)
    img = Image.open(BytesIO(requests.get(url).content))

#Prediction on the new photo
    prediction = model_2.predict(img_array)
    predicted_class = np.argmax(prediction, axis=1)

#Print the photo and the prediction
# 1= automobile, 5=dog, 6=frog
    plt.imshow(img)
    plt.title(f"Image {i+1} prediction: {predicted_class}")
    plt.axis('off')
    plt.show()

"""<div style="border-radius:12px; padding: 20px; background-color: #e2c9ff; font-size:120%; text-align:left">
    
As seen, the model correctly predicted this single desired image.

<div style="display: flex; align-items: center; justify-content: center; border-radius: 10px; padding: 20px; background-color:  #e2c9ff; font-size: 120%; text-align: center;">

<strong>🎯 If you need more information or want to explore the code, feel free to visit the project repository on <a href="https://github.com/FarzadNekouee/Keras-CIFAR10-CNN-Model">GitHub</a> 🎯</strong>
</div>

<h2 align="left"><font color='#8502d1'>Best Regards!</font></h2>
"""